{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTEBOOK FOR RUNNING REFRESH STRATEGY EXPERIMENTS ON THE GRIDWORLD SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify BASE_PATH as per user needs\n",
    "BASE_PATH = \"D:/Aman/IIIT Delhi/IP-RL/GridWorldExperiments/Logs/\"\n",
    "# TIMOUT is the max steps in an episode run\n",
    "TIMEOUT = 5000\n",
    "EPISODES = 700\n",
    "# Control the episodic frequency of decay of the learning rate \"alpha\"\n",
    "DECAY_FREQ = 500\n",
    "# Controls the frequency in terms of steps when a snapshot of the replay buffer should be stored if enabled\n",
    "MEMORY_VISUALIZE_FREQ = {100:100,1000:250,10000:1000} # Visualization frequency for different buffer sizes\n",
    "# Dimensions of the grid [Note we use a square grid]\n",
    "grid_dim = 64\n",
    "# Dictionary that maps the nmeric action to its physical meaning\n",
    "action_map = {0:\"R\",1:\"D\",2:\"L\",3:\"U\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class is responsible for creating nd managing the environment and the state of the agent in it.\n",
    "\"\"\"\n",
    "class Environment:\n",
    "    \n",
    "    def __init__(self, gridsize):        \n",
    "        # Constants\n",
    "        self.gridsize = gridsize \n",
    "        self.start_state = (self.gridsize-2,self.gridsize-2)\n",
    "        self.terminal_states = [(self.gridsize//2,self.gridsize//2)] # Terminal states\n",
    "        self.terminal_rewards = 0\n",
    "        self.actions = [[0,1],[1,0],[0,-1],[-1,0]]  # Right,Down,Left,Up\n",
    "        self.non_term_reward = -1\n",
    "        \n",
    "        # Grid Setup\n",
    "        self.grid = np.ones((gridsize,gridsize))\n",
    "        self.make_grid()\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    \"\"\"\n",
    "    Creates the game grid. The values in the grid have the following meaning:\n",
    "    1 - Free tile. An agent can move here\n",
    "    0 - A wall. An agent cannot land on this tile\n",
    "    2 - The final state. An episode terminates when an agent reaches here. \n",
    "    \"\"\"\n",
    "    def make_grid(self):\n",
    "        for i in range(self.gridsize//4,3*(self.gridsize//4)):\n",
    "            self.grid[i,self.gridsize//4] = 0\n",
    "            self.grid[i,3*(self.gridsize//4)-1] = 0\n",
    "            \n",
    "        for i in range(self.gridsize//4,3*(self.gridsize//4)):\n",
    "            self.grid[self.gridsize//4,i] = 0\n",
    "            self.grid[3*(self.gridsize//4)-1,i] = 0\n",
    "            \n",
    "        for i in range(self.gridsize//4,int(1.5*(self.gridsize//4))):\n",
    "            self.grid[i,self.gridsize//4] = 1\n",
    "            \n",
    "        # Terminal State\n",
    "        for state in self.terminal_states:\n",
    "            self.grid[state] = 2\n",
    "                \n",
    "    def __get_neighbours(self,state):\n",
    "        neighbours = []\n",
    "        for action in self.actions:\n",
    "            next_state = (state[0]+action[0],state[1]+action[1])\n",
    "            # Checking for out of grid state\n",
    "            if next_state[0]<0 or next_state[1]<0 or next_state[0]>=self.gridsize or next_state[1]>=self.gridsize:\n",
    "                continue\n",
    "            # Check if next state is a wall\n",
    "            if self.grid[next_state] == 0:\n",
    "                continue\n",
    "            neighbours.append(next_state)\n",
    "        return neighbours\n",
    "        \n",
    "    \"\"\"\n",
    "    Function that computes optimal Q calue for a given grid. \n",
    "    Computes optimal state values using BFS since the reward is -1 for every step.\n",
    "    \"\"\"\n",
    "    def compute_optimal_q_values(self):\n",
    "        self.optimal_q = np.ones((self.gridsize,self.gridsize,len(self.actions)))\n",
    "        self.optimal_v = -1*np.ones((self.gridsize,self.gridsize))\n",
    "        for term in self.terminal_states:\n",
    "            queue = [term]\n",
    "            self.optimal_v[term] = 0\n",
    "            reward = None \n",
    "            \n",
    "            while len(queue) > 0:\n",
    "                cur = queue.pop(0)\n",
    "                if cur in self.terminal_states:\n",
    "                    reward = self.terminal_rewards\n",
    "                else:\n",
    "                    reward = -self.non_term_reward\n",
    "                    \n",
    "                for neigh in self.__get_neighbours(cur):\n",
    "                    updated_v = self.optimal_v[cur] + reward\n",
    "                    if self.optimal_v[neigh] < 0:\n",
    "                        self.optimal_v[neigh] = updated_v\n",
    "                        queue.append(neigh)\n",
    "                    elif self.optimal_v[neigh] > updated_v:\n",
    "                        self.optimal_v[neigh] = updated_v\n",
    "                        \n",
    "        self.optimal_v = -1*self.optimal_v\n",
    "        \n",
    "        for i in range(self.gridsize):\n",
    "            for j in range(self.gridsize):\n",
    "                for a in range(len(self.actions)):\n",
    "                    if self.grid[(i,j)] != 0:\n",
    "                        self.current_state = (i,j)\n",
    "                        reward, next_state, _ = self.step(a)\n",
    "                        self.optimal_q[i,j,a] = self.optimal_v[next_state] + reward\n",
    "                        \n",
    "        self.reset()\n",
    "        return self.optimal_q\n",
    "                        \n",
    "            \n",
    "    def get_steps(self):\n",
    "        return self.env_steps\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        if state in self.terminal_states: \n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_state = self.start_state\n",
    "        self.env_steps = 0\n",
    "        return self.current_state\n",
    "         \n",
    "    def render(self):\n",
    "        \n",
    "        # get map value at current location\n",
    "        cur_map_value = self.grid[self.current_state]\n",
    "        \n",
    "        # Mark current location on map\n",
    "        self.grid[self.current_state] = 3\n",
    "        \n",
    "        # make a color map of fixed colors\n",
    "        cmap = mpl.colors.ListedColormap(['orange','cyan','purple','red'])\n",
    "        bounds=[0,0.5,1.5,2.5,3]\n",
    "        norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "        # tell imshow about color map so that only set colors are used\n",
    "        img = plt.imshow(self.grid ,interpolation='nearest', cmap = cmap,norm=norm)\n",
    "        # make a color bar\n",
    "        plt.colorbar(img,cmap=cmap,norm=norm,boundaries=bounds,ticks=[-5,0,5])\n",
    "        plt.show()\n",
    "        \n",
    "        # Resetting original map value at current location\n",
    "        self.grid[self.current_state] = cur_map_value\n",
    "        \n",
    "    \"\"\"\n",
    "    This function retrieves the reward and next state for an agent performing an action in its current position\n",
    "    \"\"\"\n",
    "    def step(self, action):\n",
    "        \n",
    "        # updating episode step count\n",
    "        self.env_steps+=1\n",
    "        \n",
    "        # Checking for special states\n",
    "        if self.is_terminal(self.current_state):\n",
    "            return self.terminal_rewards,self.current_state,True\n",
    "        \n",
    "        # Calculating next state\n",
    "        next_state = (self.current_state[0]+self.actions[action][0],self.current_state[1]+self.actions[action][1])\n",
    "        \n",
    "        # Checking for out of grid state\n",
    "        if next_state[0]<0 or next_state[1]<0 or next_state[0]>=self.gridsize or next_state[1]>=self.gridsize:\n",
    "            return self.non_term_reward,self.current_state,False\n",
    "        \n",
    "        # Check if next state is a wall\n",
    "        if self.grid[next_state] == 0:\n",
    "            return self.non_term_reward,self.current_state,False\n",
    "        \n",
    "        # Returning next state and reward\n",
    "        self.current_state = next_state\n",
    "        return self.non_term_reward,next_state,False\n",
    "    \n",
    "    \"\"\"\n",
    "    This function retrieves the reward and next state for an arbitrary current state and action. \n",
    "    This is required for the Refresh strategy\n",
    "    \"\"\"\n",
    "    def step_async(self, state, action):\n",
    "        self.env_steps -= 1\n",
    "        actual_cur_state, self.current_state = self.current_state, state\n",
    "        reward, next_state, done = self.step(action)\n",
    "        self.current_state = actual_cur_state\n",
    "        return reward, next_state, done\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(grid_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25ca73dde80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implemention of a cicular replay buffer with uniform sampling\n",
    "\"\"\"\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, buffer_size=100, batch_sz=10):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_sz = batch_sz\n",
    "        self.pointer = 0\n",
    "        self.cur_size = 0\n",
    "        self.buffer = [0 for i in range(buffer_size)]\n",
    "        self.quorom = min(buffer_size/10,10000)\n",
    "        self.hist = {\"x\":[],\"y\":[]}\n",
    "        \n",
    "    def at_quorom(self):\n",
    "        return self.cur_size >= self.quorom\n",
    "        \n",
    "    def add(self,transition):\n",
    "        state,_,_,_,_ = transition\n",
    "        x,y = state\n",
    "        if self.cur_size < self.buffer_size:\n",
    "            self.hist[\"x\"].append(x)\n",
    "            self.hist[\"y\"].append(y)\n",
    "        else:\n",
    "            self.hist[\"x\"][self.pointer] = x\n",
    "            self.hist[\"y\"][self.pointer] = y\n",
    "        self.buffer[self.pointer] = transition\n",
    "        self.pointer = (self.pointer+1)%self.buffer_size\n",
    "        self.cur_size = min(self.cur_size+1,self.buffer_size)\n",
    "        \n",
    "    def sample(self):\n",
    "        idx = random.randint(0,self.cur_size-1)\n",
    "        transition = self.buffer[idx]\n",
    "        return idx, transition\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        batch = []\n",
    "        indices = []\n",
    "        while len(batch) < self.batch_sz:\n",
    "            idx, transition = self.sample()\n",
    "            if idx not in indices:\n",
    "                indices.append(idx)\n",
    "                batch.append(transition)\n",
    "        return batch\n",
    "    \n",
    "    def get_buffer_histogram(self):\n",
    "#         plt.hist2d(self.hist[\"x\"], self.hist[\"y\"], bins =[grid_dim, grid_dim], range=[[0, grid_dim-1], [0, grid_dim-1]]) \n",
    "#         plt.tight_layout()  \n",
    "#         plt.show() \n",
    "        return self.hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class responsible for logging\n",
    "\"\"\"\n",
    "class Logger:\n",
    "    \n",
    "    def __init__(self,path,name):\n",
    "        self.path = BASE_PATH + path\n",
    "        self.name = name\n",
    "        self.font = cv2.FONT_HERSHEY_PLAIN\n",
    "        self.visualize_mem_steps = 0\n",
    "        self.logs={}\n",
    "        self.create()\n",
    "        self.setup()\n",
    "        \n",
    "    def create(self):\n",
    "        os.makedirs(self.path,exist_ok=True)\n",
    "        os.makedirs(self.path+self.name.split(\".\")[0]+\"-memory/\",exist_ok=True)\n",
    "        open(self.path+self.name, 'ab').close()\n",
    "        \n",
    "    def make_plot(self):\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.set_title(\"Train Log - Epsiode Returns\")\n",
    "        ax1.plot(self.logs[\"returns\"])\n",
    "        ax1.set_xlabel(\"Episodes\")\n",
    "        ax1.set_ylabel(\"Returns\")\n",
    "        fig1.savefig(self.path+self.name+'-returns.png',dpi=800)\n",
    "        plt.clf()\n",
    "        \n",
    "        fig2, ax2 = plt.subplots()\n",
    "        ax2.set_title(\"Train Log - Epsiode Variance\")\n",
    "        ax2.plot(self.logs[\"variance\"])\n",
    "        ax2.set_xlabel(\"Episodes\")\n",
    "        ax2.set_ylabel(\"Variance\")\n",
    "        fig2.savefig(self.path+self.name+'-variance.png',dpi=800)\n",
    "        plt.clf()\n",
    "        \n",
    "        fig3, ax3 = plt.subplots()\n",
    "        ax3.set_title(\"Train Log - Epsiode Entropy\")\n",
    "        ax3.plot(self.logs[\"entropy\"])\n",
    "        ax3.set_xlabel(\"Episodes\")\n",
    "        ax3.set_ylabel(\"Entropy\")\n",
    "        fig3.savefig(self.path+self.name+'-entropy.png',dpi=800)\n",
    "        plt.clf()\n",
    "        \n",
    "        fig4, ax4 = plt.subplots()\n",
    "        ax4.set_title(\"Train Log - Epsiode Running Variance\")\n",
    "        ax4.plot(self.logs[\"running_variance\"])\n",
    "        ax4.set_xlabel(\"Episodes\")\n",
    "        ax4.set_ylabel(\"Running Variance\")\n",
    "        fig4.savefig(self.path+self.name+'-runningvariance.png',dpi=800)\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "    def setup(self):\n",
    "        self.logs[\"returns\"] = []\n",
    "        self.logs[\"variance\"] = []\n",
    "        self.logs[\"running_variance\"] = []\n",
    "        self.logs[\"entropy\"] = []\n",
    "        \n",
    "    def step(self,ep_stats):\n",
    "        self.logs[\"returns\"].append(ep_stats[\"return\"])\n",
    "        self.logs[\"variance\"].append(ep_stats[\"variance\"])\n",
    "        self.logs[\"running_variance\"].append(ep_stats[\"running_variance\"])\n",
    "        self.logs[\"entropy\"].append(ep_stats[\"entropy\"])\n",
    "        \n",
    "    def save(self):\n",
    "        if len(self.logs[\"returns\"])%EPISODES==0 and len(self.logs[\"returns\"])>0:\n",
    "            self.make_plot()\n",
    "        with open(self.path+self.name,\"wb\") as f:\n",
    "            pickle.dump(self.logs,f)\n",
    "            \n",
    "    def save_replay_snapshot(self,histogram,episode):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist2d(histogram[\"x\"], histogram[\"y\"], bins =[grid_dim, grid_dim], range=[[0, grid_dim-1], [0,grid_dim-1]])\n",
    "        fig.gca().invert_yaxis()\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        cv2.putText(data, \"Episode=\"+str(episode), (5,15), self.font, 1, (0, 255, 0), 1,)\n",
    "        \n",
    "        im_name = str(self.visualize_mem_steps)\n",
    "        im_name = (6-len(im_name))*\"0\" + im_name\n",
    "        save_path = self.path+self.name.split(\".\")[0]+\"-memory/\"+im_name+'.jpg'\n",
    "        cv2.imwrite(save_path,data)\n",
    "        \n",
    "        self.visualize_mem_steps += 1\n",
    "        plt.close('all')\n",
    "            \n",
    "    def load(self):\n",
    "        self.logs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The agent class containing the current policy and methods for enacting greedy and epsilon greedy actions\n",
    "\"\"\"\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self,state_space=(128,128),action_space=4,epsilon=0.1,decay_factor=0.7,decay_freq=100,base_epsilon=0.8,min_epsilon=0.1):\n",
    "        self.policy = np.zeros((*state_space,action_space))\n",
    "        self.action_space = action_space\n",
    "        self.epsilon = epsilon\n",
    "        self.decay_factor = decay_factor\n",
    "        self.decay_freq = decay_freq\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.base_epsilon = base_epsilon\n",
    "        \n",
    "    def update_epsilon(self,step):\n",
    "        self.epsilon = max(self.base_epsilon*self.decay_factor**(step//self.decay_freq),self.min_epsilon)\n",
    "        \n",
    "    def greedy_action(self,state):\n",
    "        x,y = state\n",
    "        return np.argmax(self.policy[x,y,:])\n",
    "    \n",
    "    def epsilon_greedy_action(self,state):\n",
    "        x,y = state\n",
    "        greedy = np.random.uniform() > self.epsilon\n",
    "        if greedy:\n",
    "            return np.argmax(self.policy[x,y,:])\n",
    "        return np.random.randint(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class tracks the Q value at every (S,A) for a duration of max history and using this computes the \n",
    "running mean and variance and update\n",
    "\"\"\"\n",
    "class QTracker:\n",
    "    \n",
    "    def __init__(self,q_opt,state_space=(128,128),action_space=4,max_history=50):\n",
    "        self.max_history = max_history\n",
    "        self.q_history = np.zeros((*state_space,action_space,max_history))\n",
    "        self.q_pointer = np.ones((*state_space,action_space,2))\n",
    "        self.q_opt = q_opt\n",
    "        \n",
    "    def step(self, q_value, state, action):\n",
    "        ptr_idx = (state[0],state[1],action,0)\n",
    "        history_idx = (state[0],state[1],action,int(self.q_pointer[ptr_idx]%self.max_history))\n",
    "\n",
    "        self.q_history[history_idx]=q_value\n",
    "        self.q_pointer[ptr_idx] = (self.q_pointer[ptr_idx]+1)%self.max_history\n",
    "        if self.q_pointer[ptr_idx] == 0:\n",
    "            self.q_pointer[state[0],state[1],action,1] = 0 # implies that max_history samples have been populated\n",
    "        \n",
    "    def get_running_mean(self,state,action):\n",
    "        samples = []\n",
    "        if self.q_pointer[(*state,action,1)] == 1:\n",
    "            samples = self.q_history[state[0],state[1],action,0:int(self.q_pointer[state[0],state[1],action,0])]\n",
    "        else:\n",
    "            samples = self.q_history[state[0],state[1],action,:]\n",
    "        expected_target = np.sum(samples)\n",
    "        return expected_target\n",
    "    \n",
    "    def get_optimal_mean(self,state,action):\n",
    "        return self.q_opt[state[0],state[1],action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The driver code for training an agent using single step returns.\n",
    "The meaning of the config parameters are described in the config dict after the treeRunner implementation below.\n",
    "\"\"\"\n",
    "class Runner:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.n = config[\"n\"]\n",
    "        self.batch_sz = config[\"batch_sz\"]\n",
    "        self.alpha = config[\"alpha\"]\n",
    "        self.epsilon = config[\"epsilon\"]\n",
    "        self.convergence_threshold = config[\"convergence_threshold\"]\n",
    "        self.buffer_sz = config[\"buffer_sz\"]\n",
    "        self.gridsize = config[\"gridsize\"]\n",
    "        self.max_history = config[\"max_history\"]\n",
    "        self.visualize_memory = config[\"visualize_memory\"]\n",
    "        self.experiment = config[\"experiment\"]\n",
    "        self.action_space = config['action_space']\n",
    "        self.noise_prob = config[\"noise_prob\"]\n",
    "        self.config = config\n",
    "        self.ep_stats = {}\n",
    "        self.n_step_history = [0 for i in range(self.n+1)]\n",
    "        self.n_step_ptr = 0\n",
    "        self.s1,self.s2,self.s3=0,0,0\n",
    "        \n",
    "    def setup(self):\n",
    "        self.agent = Agent(state_space=(self.gridsize,self.gridsize),action_space=self.config['action_space'], epsilon=self.epsilon)\n",
    "        self.memory = ReplayBuffer(buffer_size=self.buffer_sz,batch_sz=self.batch_sz)\n",
    "        self.env = Environment(gridsize=self.gridsize)\n",
    "        self.logger = Logger(self.experiment+\"/\"+self.method+\"/\"+str(self.n)+\"/\",str(self.buffer_sz)+\".pickle\")\n",
    "        self.q_tracker = QTracker(self.env.compute_optimal_q_values(),state_space=(self.gridsize,self.gridsize),action_space=self.config['action_space'],max_history=self.config[\"max_history\"])  \n",
    "        \n",
    "    def decay_alpha(self,eps):\n",
    "        self.alpha = max(self.alpha*(0.7**(eps//DECAY_FREQ)),0.0001)\n",
    "        \n",
    "    def decay_noise(self,eps):\n",
    "        self.noise_prob =  self.config[\"noise_prob\"]*(self.config[\"noise_decay\"]**(eps//self.config[\"noise_decay_freq\"]))\n",
    "        \n",
    "    def reset_episode_log(self):\n",
    "        self.ep_stats[\"return\"] = 0\n",
    "        self.ep_stats[\"variance\"] = 0\n",
    "        self.ep_stats[\"running_variance\"] = 0\n",
    "        self.ep_stats[\"entropy\"] = 0\n",
    "        self.ep_stats[\"train_step\"] = 0\n",
    "        \n",
    "    def update_episode_log(self,reward,variance,running_variance,entropy):\n",
    "        self.ep_stats[\"train_step\"] += 1\n",
    "        self.ep_stats[\"return\"] += reward\n",
    "        self.ep_stats[\"variance\"] += variance\n",
    "        self.ep_stats[\"running_variance\"] += running_variance\n",
    "        self.ep_stats[\"entropy\"] += entropy\n",
    "        \n",
    "    def process_episode_log(self):\n",
    "        self.ep_stats[\"running_variance\"] = self.ep_stats[\"running_variance\"]/self.ep_stats[\"train_step\"]\n",
    "        self.ep_stats[\"variance\"] = self.ep_stats[\"variance\"]/self.ep_stats[\"train_step\"]\n",
    "        self.ep_stats[\"entropy\"] = self.ep_stats[\"entropy\"]/self.ep_stats[\"train_step\"]     \n",
    "        \n",
    "    def reset_n_step_buffer(self):\n",
    "        self.n_step_history = [0 for i in range(self.n+1)]\n",
    "        self.n_step_ptr = 0\n",
    "        \n",
    "    def update_n_step_buffer(self,action,next_state,reward):\n",
    "        self.n_step_history[self.n_step_ptr] = (next_state,action,reward)\n",
    "        self.n_step_ptr = (self.n_step_ptr+1)%(self.n+1)\n",
    "        \n",
    "    def save_q_values(self):\n",
    "        with open(BASE_PATH+self.experiment+\"/\"+self.method+\"/\"+str(self.n)+\"/\"+str(self.buffer_sz)+\"-q.pickle\",\"wb\") as f:\n",
    "            pickle.dump(self.agent.policy,f)\n",
    "            \n",
    "    def get_action_probability(self,state,action):\n",
    "        if action == np.argmax(self.agent.policy[state[0],state[1],:]):\n",
    "            return 1 - self.agent.epsilon + self.agent.epsilon/self.agent.action_space\n",
    "        return self.agent.epsilon/self.agent.action_space\n",
    "    \n",
    "    def get_entropy(self,q_vec):\n",
    "        entropy = 0\n",
    "        abs_max = float('-inf')\n",
    "        for i in range(q_vec.shape[0]):\n",
    "            if abs(q_vec[i]) > abs_max:\n",
    "                abs_max = abs(q_vec[i])\n",
    "                \n",
    "        for i in range(q_vec.shape[0]):\n",
    "            norm = q_vec[i] / (abs_max+10**-5)\n",
    "            entropy += -norm*math.exp(norm)\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    def get_variance(self,state,action,target,q_val):\n",
    "        self.q_tracker.step(q_val,state,action)\n",
    "        running_expectation = self.q_tracker.get_running_mean(state,action)\n",
    "        expectation = self.q_tracker.get_optimal_mean(state,action)\n",
    "        variance = (target-expectation)**2\n",
    "        running_variance = (target-running_expectation)**2\n",
    "        return variance, running_variance\n",
    "            \n",
    "    def learn(self):\n",
    "        for eps in tqdm(range(EPISODES)):\n",
    "            cur_state = self.env.reset()\n",
    "            self.reset_episode_log()\n",
    "            done = False\n",
    "            self.reset_n_step_buffer()\n",
    "            self.update_n_step_buffer(0,cur_state,-1)\n",
    "            self.decay_alpha(eps)\n",
    "            while not done and self.env.get_steps()<TIMEOUT:\n",
    "                action = self.agent.epsilon_greedy_action(cur_state)\n",
    "                if not self.memory.at_quorom():\n",
    "                    action = random.randint(0,3)\n",
    "                reward, next_state, done = self.env.step(action)\n",
    "                self.update_n_step_buffer(action,next_state,reward)\n",
    "                transition = (*self.make_transition(),done)\n",
    "                self.memory.addx(transition)\n",
    "                if not self.memory.at_quorom():\n",
    "                    continue\n",
    "                batch = self.memory.sample_batch()\n",
    "                variance,running_variance,entropy = self.train_on_batch(batch)    \n",
    "                cur_state = next_state\n",
    "                self.update_episode_log(reward,variance,running_variance,entropy)\n",
    "    \n",
    "            self.process_episode_log()\n",
    "            self.logger.step(self.ep_stats)\n",
    "            self.logger.save()\n",
    "        self.save_q_values()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The driver code for training an agent using multi step returns\n",
    "\"\"\"\n",
    "class nStepRunner(Runner):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.method = \"TD\"\n",
    "        Runner.__init__(self,config)\n",
    "        \n",
    "    def uncorrected_n_step(self,start_ptr):\n",
    "        reward = 0\n",
    "        cur_gamma = 1\n",
    "        for ptr in range(start_ptr+1,start_ptr+1+self.n):\n",
    "            _,state,r = self.n_step_history[ptr%(self.n+1)]\n",
    "            if state is None: # Truncate reward as goal reached\n",
    "                break\n",
    "            reward += r*cur_gamma\n",
    "            cur_gamma = cur_gamma*self.gamma\n",
    "        return reward\n",
    "           \n",
    "    def make_transition(self):\n",
    "        start_ptr = (self.n_step_ptr)%(self.n+1)\n",
    "        start_state,_,_ = self.n_step_history[start_ptr]\n",
    "        _,action,_ = self.n_step_history[(start_ptr+1)%(self.n+1)]\n",
    "        reward = self.uncorrected_n_step(start_ptr)\n",
    "        final_state,_,_ = self.n_step_history[(start_ptr-1)%(self.n+1)]\n",
    "        transition = (start_state,action,reward,final_state)\n",
    "        return transition\n",
    "        \n",
    "    def get_TD_n_target(self,reward,q_next):\n",
    "        target = reward + (self.gamma**self.n)*q_next\n",
    "        return target     \n",
    "    \n",
    "    \"\"\"\n",
    "    Function responsible for refreshing transition that was just replayed\n",
    "    \"\"\"\n",
    "    def refresh_transition(self,state):\n",
    "        cur_state = state\n",
    "        cur_action, next_state, done = None, None, False\n",
    "        reward, cur_gamma = 0, 1\n",
    "        for step in range(self.n):\n",
    "            action = self.agent.epsilon_greedy_action(cur_state)\n",
    "            if step == 0:\n",
    "                cur_action = action\n",
    "            r, next_state, done = self.env.step_async(cur_state,action)\n",
    "            if \"Noisy\" in self.experiment:\n",
    "                if random.random() < self.noise_prob:\n",
    "                    r, next_state, done = self.env.step_async(cur_state,random.randint(0,self.config[\"action_space\"]-1))\n",
    "            reward += r*cur_gamma\n",
    "            cur_gamma = cur_gamma*self.gamma\n",
    "            cur_state = next_state\n",
    "        return cur_action, reward, next_state, done\n",
    "    \n",
    "    def train_on_batch(self,batch):\n",
    "        variance,running_variance,entropy = 0,0,0\n",
    "        for transition in batch:\n",
    "            cur_state,action,reward,next_state,done = transition\n",
    "            if \"Refresh\" in self.experiment:\n",
    "                action,reward,next_state,done = self.refresh_transition(cur_state)\n",
    "            q_cur = self.agent.policy[cur_state[0],cur_state[1],action]\n",
    "            q_next = 0\n",
    "            if next_state is not None: # Episode in progress, No truncation needed\n",
    "                next_action = self.agent.greedy_action(next_state)\n",
    "                q_next = self.agent.policy[next_state[0],next_state[1],next_action]\n",
    "            target = self.get_TD_n_target(reward,q_next)\n",
    "            self.agent.policy[cur_state[0],cur_state[1],action] = q_cur + self.alpha*(target - q_cur)\n",
    "            # Updating stats\n",
    "            if next_state is not None:\n",
    "                entropy += self.get_entropy(self.agent.policy[next_state[0],next_state[1],:])\n",
    "            v, r_v = self.get_variance(cur_state,action,target,self.agent.policy[cur_state[0],cur_state[1],action])\n",
    "            variance += v\n",
    "            running_variance += r_v\n",
    "            \n",
    "        return variance/len(batch),running_variance//len(batch),entropy/len(batch)\n",
    "        \n",
    "    def learn_n_step(self):\n",
    "                \n",
    "        total_steps = 0\n",
    "            \n",
    "        for eps in tqdm(range(EPISODES)):\n",
    "            cur_state = self.env.reset()\n",
    "            self.reset_episode_log()\n",
    "            self.decay_alpha(eps)\n",
    "            self.decay_noise(eps)\n",
    "            self.reset_n_step_buffer()\n",
    "            self.update_n_step_buffer(0,cur_state,-1)\n",
    "            T = TIMEOUT\n",
    "            done = False\n",
    "            reward = 0\n",
    "            \n",
    "            while True:\n",
    "                                \n",
    "                time_step = self.env.get_steps()\n",
    "                \n",
    "                if time_step == TIMEOUT:\n",
    "                    break\n",
    "                    \n",
    "                # Not timeout or termination\n",
    "                if time_step < T:\n",
    "                    action = self.agent.epsilon_greedy_action(cur_state)\n",
    "                    if not self.memory.at_quorom():\n",
    "                        action = random.randint(0,3)\n",
    "                    reward, next_state, done = self.env.step(action)\n",
    "                    cur_state = next_state\n",
    "                    self.update_n_step_buffer(action,next_state,reward)\n",
    "                    # reached terminal state\n",
    "                    if done:\n",
    "                        T = time_step\n",
    "                    # incrementing total env steps\n",
    "                    total_steps += 1\n",
    "                \n",
    "                # Not timeout\n",
    "                elif time_step != TIMEOUT:\n",
    "                    time_step += 1    \n",
    "                    self.update_n_step_buffer(-1,None,0)\n",
    "                    \n",
    "                if self.visualize_memory and total_steps%MEMORY_VISUALIZE_FREQ[self.buffer_sz]==0:\n",
    "                    memory_histogram = self.memory.get_buffer_histogram()\n",
    "                    self.logger.save_replay_snapshot(memory_histogram,eps+1)\n",
    "                        \n",
    "                tau = time_step - (self.n - 1)\n",
    "                \n",
    "                if tau >= 0:\n",
    "                    transition = (*self.make_transition(),done)\n",
    "                    self.memory.add(transition)\n",
    "                    \n",
    "                    if not self.memory.at_quorom():\n",
    "                        continue\n",
    "                    \n",
    "                    batch = self.memory.sample_batch()\n",
    "                    variance,running_variance,entropy = self.train_on_batch(batch)\n",
    "                    self.update_episode_log(reward,variance,running_variance,entropy)\n",
    "                    \n",
    "                if tau >= T:\n",
    "                    break\n",
    "                                             \n",
    "\n",
    "            if not self.memory.at_quorom():\n",
    "                continue\n",
    "                    \n",
    "            self.process_episode_log()\n",
    "            self.logger.step(self.ep_stats)\n",
    "            self.logger.save()\n",
    "        self.save_q_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Backup - State Trajectory and Replay Transition Scheme\n",
    "\n",
    "### Given reward steps - _n_ and  current state - _S0_\n",
    "Current Action = A0 <br>\n",
    "State Trajectory = { (S1,A1), (S2,A2), ... , (Sn) } <br>\n",
    "Rewards = { R1, R2, .... Rn }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class treeRunner(Runner):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        self.method = \"tree\"\n",
    "        Runner.__init__(self,config)\n",
    "        \n",
    "    def refresh_transition(self,cur_state):\n",
    "        state = cur_state\n",
    "        action = self.agent.epsilon_greedy_action(state)\n",
    "        rewards = []\n",
    "        state_trajectory = []\n",
    "        \n",
    "        for step in range(self.n): \n",
    "            reward, next_state, done = self.env.step_async(state,action)\n",
    "            if \"Noisy\" in self.experiment:\n",
    "                if random.random() < self.noise_prob:\n",
    "                    reward, next_state, done = self.env.step_async(state,random.randint(0,self.config[\"action_space\"]-1))\n",
    "            next_action = self.agent.epsilon_greedy_action(next_state)\n",
    "            rewards.append(reward)\n",
    "            state_trajectory.append([next_state,next_action])\n",
    "            \n",
    "        state_trajectory[-1] = [state_trajectory[-1][0]]\n",
    "        return action,rewards,state_trajectory,done\n",
    "        \n",
    "    def make_transition(self):\n",
    "        start_ptr = (self.n_step_ptr)%(self.n+1)\n",
    "        start_state,_,_ = self.n_step_history[start_ptr]\n",
    "        _,action,_ = self.n_step_history[(start_ptr+1)%(self.n+1)]\n",
    "        rewards = []\n",
    "        state_trajectory = []\n",
    "        for ptr in range(start_ptr+1,start_ptr+1+self.n):\n",
    "            state,prev_action,r = self.n_step_history[ptr%(self.n+1)]\n",
    "            if state is None: # Truncate reward as goal reached\n",
    "                break\n",
    "            if len(state_trajectory) > 0:\n",
    "                state_trajectory[-1].append(prev_action)\n",
    "            rewards.append(r)\n",
    "            state_trajectory.append([state])           \n",
    "        transition = (start_state,action,rewards,state_trajectory)\n",
    "        return transition\n",
    "    \n",
    "    # Σa π(a|s)Q(s,a)\n",
    "    def get_state_expectation(self,state): \n",
    "        expectation = 0\n",
    "        for action in range(4):\n",
    "            expectation += self.get_action_probability(state,action)*self.agent.policy[state[0],state[1],action]\n",
    "        return expectation\n",
    "        \n",
    "    def get_tree_target(self,rewards,state_trajectory):\n",
    "        target = 0\n",
    "        cur_gamma = 1\n",
    "        policy_prob = 1\n",
    "        for lvl in range(len(state_trajectory)):\n",
    "            target += cur_gamma*policy_prob*rewards[lvl]\n",
    "            cur_gamma *= self.gamma   \n",
    "            next_state = state_trajectory[lvl][0]\n",
    "            if next_state is None: # truncate target since goal has been reached\n",
    "                break \n",
    "            elif len(state_trajectory[lvl]) == 1: # final state\n",
    "                target += cur_gamma*policy_prob*self.get_state_expectation(next_state)\n",
    "            else: # intermediate states\n",
    "                next_action = state_trajectory[lvl][1]\n",
    "                target += cur_gamma*policy_prob*self.get_state_expectation(next_state)\n",
    "                policy_prob *= self.get_action_probability(next_state,next_action)\n",
    "                target -= cur_gamma*policy_prob*self.agent.policy[next_state[0],next_state[1],next_action]\n",
    "\n",
    "        return target     \n",
    "    \n",
    "    def train_on_batch(self,batch):\n",
    "        variance,running_variance,entropy,ctr = 0,0,0,0\n",
    "        for transition in batch:\n",
    "            cur_state,action,rewards,state_trajectory,done = transition\n",
    "            if \"Refresh\" in self.experiment:\n",
    "                action,rewards,state_trajectory,done = self.refresh_transition(cur_state)\n",
    "            q_cur = self.agent.policy[cur_state[0],cur_state[1],action]\n",
    "            final_state = state_trajectory[-1][0]\n",
    "            target = self.get_tree_target(rewards,state_trajectory)\n",
    "            self.agent.policy[cur_state[0],cur_state[1],action] = q_cur + self.alpha*(target - q_cur)\n",
    "            # Updating stats\n",
    "            if final_state is not None:\n",
    "                entropy += self.get_entropy(self.agent.policy[final_state[0],final_state[1],:])\n",
    "            v, r_v = self.get_variance(cur_state,action,target,self.agent.policy[cur_state[0],cur_state[1],action])\n",
    "            variance += v\n",
    "            running_variance += r_v\n",
    "            ctr += 1\n",
    "            \n",
    "        return variance/ctr,running_variance/ctr,entropy/ctr\n",
    "        \n",
    "    def learn_n_step(self):\n",
    "        \n",
    "        total_steps = 0\n",
    "                \n",
    "        for eps in tqdm(range(EPISODES)):\n",
    "            cur_state = self.env.reset()\n",
    "            self.reset_episode_log()\n",
    "            self.decay_alpha(eps)\n",
    "            self.decay_noise(eps)\n",
    "            self.reset_n_step_buffer()\n",
    "            self.update_n_step_buffer(0,cur_state,-1)\n",
    "            \n",
    "            T = TIMEOUT\n",
    "            done = False\n",
    "            reward = 0\n",
    "            \n",
    "            while True:\n",
    "                                \n",
    "                time_step = self.env.get_steps()\n",
    "                \n",
    "                if time_step == TIMEOUT:\n",
    "                    break\n",
    "                    \n",
    "                # Not timeout or termination\n",
    "                if time_step < T:\n",
    "                    action = self.agent.epsilon_greedy_action(cur_state)\n",
    "                    if not self.memory.at_quorom():\n",
    "                        action = random.randint(0,3)\n",
    "                    reward, next_state, done = self.env.step(action)\n",
    "                    cur_state = next_state\n",
    "                    self.update_n_step_buffer(action,next_state,reward)\n",
    "                    # reached terminal state\n",
    "                    if done:\n",
    "                        T = time_step\n",
    "                    # incrementing total env steps\n",
    "                    total_steps += 1\n",
    "                \n",
    "                # Not timeout\n",
    "                elif time_step != TIMEOUT:\n",
    "                    time_step += 1    \n",
    "                    self.update_n_step_buffer(-1,None,0)\n",
    "                    \n",
    "                if self.visualize_memory and total_steps%MEMORY_VISUALIZE_FREQ[self.buffer_sz]==0:\n",
    "                    memory_histogram = self.memory.get_buffer_histogram()\n",
    "                    self.logger.save_replay_snapshot(memory_histogram,eps+1)\n",
    "                        \n",
    "                tau = time_step - (self.n - 1)\n",
    "                \n",
    "                if tau >= 0:\n",
    "                    transition = (*self.make_transition(),done)\n",
    "                    self.memory.add(transition)\n",
    "                    \n",
    "                    if not self.memory.at_quorom():\n",
    "                        continue\n",
    "                    \n",
    "                    batch = self.memory.sample_batch()\n",
    "                    variance,running_variance,entropy = self.train_on_batch(batch)\n",
    "                    self.update_episode_log(reward,variance,running_variance,entropy)\n",
    "                    \n",
    "                if tau >= T:\n",
    "                    break\n",
    "                                             \n",
    "\n",
    "            if not self.memory.at_quorom():\n",
    "                continue\n",
    "                    \n",
    "            self.process_episode_log()\n",
    "            self.logger.step(self.ep_stats)\n",
    "            self.logger.save()\n",
    "        self.save_q_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"experiment\":\"Uniform\", # \"Uniform / Refresh / Refresh-Noisy / Small-Big\n",
    "    \"buffer_sz\":100, # 100 / 1000 / 10000\n",
    "    \"n\":2, # 1 / 2 / 3\n",
    "    \"gridsize\":grid_dim, # default = 64\n",
    "    \"visualize_memory\":False,\n",
    "    \"noise_prob\":0.5, # noise injection prob in noisy refresh regime\n",
    "    \"noise_decay\":0.5, # noise injection prob decay in noisy refresh regime\n",
    "    \"noise_decay_freq\":150,  # noise injection prob reduction frequency in noisy refresh regime\n",
    "    \"action_space\":4, # default = 4\n",
    "    \"batch_sz\":10, # default = 10\n",
    "    \"alpha\":0.1, # default = 0.1\n",
    "    \"epsilon\":0.1, # default = 0.1\n",
    "    \"gamma\":0.99, # default = 0.99\n",
    "    \"max_history\":10, # default = 10\n",
    "    \"convergence_threshold\":10**-4 # default = 10**-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24925ac0a23544298efda063967974d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25ca01fa898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25ca95155f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25ca94f5b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runner = nStepRunner(config)\n",
    "# runner = treeRunner(config)\n",
    "runner.setup()\n",
    "runner.learn_n_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70836a3871ea4221a32e3d3381f9ebc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for buff_sz in [100]:\n",
    "    for n in [3]:\n",
    "        config['n'] = n \n",
    "        config['buffer_sz'] = buff_sz\n",
    "        runner = nStepRunner(config)\n",
    "        runner.setup()\n",
    "        runner.learn_n_step()\n",
    "\n",
    "# for buff_sz in [100,1000,10000]:\n",
    "#     for n in [1,2,3]:\n",
    "#         config['n'] = n \n",
    "#         config['buffer_sz'] = buff_sz\n",
    "#         runner = treeRunner(config)\n",
    "#         runner.setup()\n",
    "#         runner.lear n_n_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-123, -132, -121, -117, -138, -138, -125, -124, -126, -124, -146, -128, -123, -119, -128, -122, -127, -122, -121, -128]\n"
     ]
    }
   ],
   "source": [
    "print(runner.logger.logs[\"returns\"][-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997]\n"
     ]
    }
   ],
   "source": [
    "print(runner.logger.logs[\"returns\"][-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in range(4):\n",
    "#     for i in range(grid_dim):\n",
    "#         for j in range(grid_dim):\n",
    "#             print(\"% 1.1f\"%(runner.agent.policy[i,j,a]),end=\" \")\n",
    "#         print()\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "path = \"D:/Aman M/Videos/Burglary/vid6/\"\n",
    "outpath = \"D:/Aman M/Videos/Burglary/vid6_enhanced/\"\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(outpath,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = glob(path+\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1965/1965 [04:20<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "for img in tqdm(frames):\n",
    "    img_name = img.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    frame = cv2.imread(img)#cv2.cvtColor(,cv2.COLOR_BGR2GRAY)\n",
    "    grey_scale = frame[:,:,0]\n",
    "    enhanced = clahe.apply(grey_scale)\n",
    "    enhanced_frame = np.zeros((grey_scale.shape[0],grey_scale.shape[1],3))\n",
    "    enhanced_frame[:,:,0] = enhanced\n",
    "    enhanced_frame[:,:,1] = enhanced\n",
    "    enhanced_frame[:,:,2] = enhanced\n",
    "    cv2.imwrite(outpath+img_name+\".jpg\",enhanced_frame)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
