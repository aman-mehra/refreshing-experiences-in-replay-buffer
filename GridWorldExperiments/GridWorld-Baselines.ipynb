{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"D:/Aman/IIIT Delhi/IP-RL/GridWorldExperiments/Logs/\"\n",
    "TIMEOUT = 5000\n",
    "EPISODES = 700\n",
    "DECAY_FREQ = 500\n",
    "MEMORY_VISUALIZE_FREQ = {100:100,1000:250,10000:1000} # Visualization frequency for different buffer sizes\n",
    "grid_dim = 64\n",
    "action_map = {0:\"R\",1:\"D\",2:\"L\",3:\"U\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self, gridsize):        \n",
    "        # Constants\n",
    "        self.gridsize = gridsize \n",
    "        self.start_state = (self.gridsize-2,self.gridsize-2)\n",
    "        self.terminal_states = [(self.gridsize//2,self.gridsize//2)] # Terminal states\n",
    "        self.terminal_rewards = 0\n",
    "        self.actions = [[0,1],[1,0],[0,-1],[-1,0]]  # Right,Down,Left,Up\n",
    "        self.non_term_reward = -1\n",
    "        \n",
    "        # Grid Setup\n",
    "        self.grid = np.ones((gridsize,gridsize))\n",
    "        self.make_grid()\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def make_grid(self):\n",
    "        for i in range(self.gridsize//4,3*(self.gridsize//4)):\n",
    "            self.grid[i,self.gridsize//4] = 0\n",
    "            self.grid[i,3*(self.gridsize//4)-1] = 0\n",
    "            \n",
    "        for i in range(self.gridsize//4,3*(self.gridsize//4)):\n",
    "            self.grid[self.gridsize//4,i] = 0\n",
    "            self.grid[3*(self.gridsize//4)-1,i] = 0\n",
    "            \n",
    "        for i in range(self.gridsize//4,int(1.5*(self.gridsize//4))):\n",
    "            self.grid[i,self.gridsize//4] = 1\n",
    "            \n",
    "        for state in self.terminal_states:\n",
    "            self.grid[state] = 2\n",
    "                \n",
    "    def __get_neighbours(self,state):\n",
    "        neighbours = []\n",
    "        for action in self.actions:\n",
    "            next_state = (state[0]+action[0],state[1]+action[1])\n",
    "            # Checking for out of grid state\n",
    "            if next_state[0]<0 or next_state[1]<0 or next_state[0]>=self.gridsize or next_state[1]>=self.gridsize:\n",
    "                continue\n",
    "            # Check if next state is a wall\n",
    "            if self.grid[next_state] == 0:\n",
    "                continue\n",
    "            neighbours.append(next_state)\n",
    "        return neighbours\n",
    "        \n",
    "    def compute_optimal_q_values(self):\n",
    "        self.optimal_q = np.ones((self.gridsize,self.gridsize,len(self.actions)))\n",
    "        self.optimal_v = -1*np.ones((self.gridsize,self.gridsize))\n",
    "        for term in self.terminal_states:\n",
    "            queue = [term]\n",
    "            self.optimal_v[term] = 0\n",
    "            reward = None \n",
    "            \n",
    "            while len(queue) > 0:\n",
    "                cur = queue.pop(0)\n",
    "                if cur in self.terminal_states:\n",
    "                    reward = self.terminal_rewards\n",
    "                else:\n",
    "                    reward = -self.non_term_reward\n",
    "                    \n",
    "                for neigh in self.__get_neighbours(cur):\n",
    "                    updated_v = self.optimal_v[cur] + reward\n",
    "                    if self.optimal_v[neigh] < 0:\n",
    "                        self.optimal_v[neigh] = updated_v\n",
    "                        queue.append(neigh)\n",
    "                    elif self.optimal_v[neigh] > updated_v:\n",
    "                        self.optimal_v[neigh] = updated_v\n",
    "                        \n",
    "        self.optimal_v = -1*self.optimal_v\n",
    "        \n",
    "        for i in range(self.gridsize):\n",
    "            for j in range(self.gridsize):\n",
    "                for a in range(len(self.actions)):\n",
    "                    if self.grid[(i,j)] != 0:\n",
    "                        self.current_state = (i,j)\n",
    "                        reward, next_state, _ = self.step(a)\n",
    "                        self.optimal_q[i,j,a] = self.optimal_v[next_state] + reward\n",
    "                        \n",
    "        self.reset()\n",
    "        return self.optimal_q\n",
    "                        \n",
    "            \n",
    "    def get_steps(self):\n",
    "        return self.env_steps\n",
    "    \n",
    "    def is_terminal(self,state):\n",
    "        if state in self.terminal_states: \n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_state = self.start_state\n",
    "        self.env_steps = 0\n",
    "        return self.current_state\n",
    "         \n",
    "    def render(self):\n",
    "        \n",
    "        # get map value at current location\n",
    "        cur_map_value = self.grid[self.current_state]\n",
    "        \n",
    "        # Mark current location on map\n",
    "        self.grid[self.current_state] = 3\n",
    "        \n",
    "        # make a color map of fixed colors\n",
    "        cmap = mpl.colors.ListedColormap(['orange','cyan','purple','red'])\n",
    "        bounds=[0,0.5,1.5,2.5,3]\n",
    "        norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "        # tell imshow about color map so that only set colors are used\n",
    "        img = plt.imshow(self.grid ,interpolation='nearest', cmap = cmap,norm=norm)\n",
    "        # make a color bar\n",
    "        plt.colorbar(img,cmap=cmap,norm=norm,boundaries=bounds,ticks=[-5,0,5])\n",
    "        plt.show()\n",
    "        \n",
    "        # Resetting original map value at current location\n",
    "        self.grid[self.current_state] = cur_map_value\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        # updating episode step count\n",
    "        self.env_steps+=1\n",
    "        \n",
    "        # Checking for special states\n",
    "        if self.is_terminal(self.current_state):\n",
    "            return self.terminal_rewards,self.current_state,True\n",
    "        \n",
    "        # Calculating next state\n",
    "        next_state = (self.current_state[0]+self.actions[action][0],self.current_state[1]+self.actions[action][1])\n",
    "        \n",
    "        # Checking for out of grid state\n",
    "        if next_state[0]<0 or next_state[1]<0 or next_state[0]>=self.gridsize or next_state[1]>=self.gridsize:\n",
    "            return self.non_term_reward,self.current_state,False\n",
    "        \n",
    "        # Check if next state is a wall\n",
    "        if self.grid[next_state] == 0:\n",
    "            return self.non_term_reward,self.current_state,False\n",
    "        \n",
    "        # Returning next state and reward\n",
    "        self.current_state = next_state\n",
    "        return self.non_term_reward,next_state,False\n",
    "    \n",
    "    def step_async(self, state, action):\n",
    "        self.env_steps -= 1\n",
    "        actual_cur_state, self.current_state = self.current_state, state\n",
    "        reward, next_state, done = self.step(action)\n",
    "        self.current_state = actual_cur_state\n",
    "        return reward, next_state, done\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(grid_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc1ef31d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, buffer_size=100, batch_sz=10):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_sz = batch_sz\n",
    "        self.pointer = 0\n",
    "        self.cur_size = 0\n",
    "        self.buffer = [0 for i in range(buffer_size)]\n",
    "        self.quorom = min(buffer_size/10,10000)\n",
    "        self.hist = {\"x\":[],\"y\":[]}\n",
    "        \n",
    "    def at_quorom(self):\n",
    "        return self.cur_size >= self.quorom\n",
    "        \n",
    "    def add(self,transition):\n",
    "        state,_,_,_,_ = transition\n",
    "        x,y = state\n",
    "        if self.cur_size < self.buffer_size:\n",
    "            self.hist[\"x\"].append(x)\n",
    "            self.hist[\"y\"].append(y)\n",
    "        else:\n",
    "            self.hist[\"x\"][self.pointer] = x\n",
    "            self.hist[\"y\"][self.pointer] = y\n",
    "        self.buffer[self.pointer] = transition\n",
    "        self.pointer = (self.pointer+1)%self.buffer_size\n",
    "        self.cur_size = min(self.cur_size+1,self.buffer_size)\n",
    "        \n",
    "    def sample(self):\n",
    "        idx = random.randint(0,self.cur_size-1)\n",
    "        transition = self.buffer[idx]\n",
    "        return idx, transition\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        batch = []\n",
    "        indices = []\n",
    "        while len(batch) < self.batch_sz:\n",
    "            idx, transition = self.sample()\n",
    "            if idx not in indices:\n",
    "                indices.append(idx)\n",
    "                batch.append(transition)\n",
    "        return batch\n",
    "    \n",
    "    def get_buffer_histogram(self):\n",
    "        return self.hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BucketedGeometricDecayReplayBuffer(ReplayBuffer):\n",
    "        \n",
    "        def __init__(self, buffer_size=100, batch_sz=10, p = 0.11, num_buckets=20):\n",
    "            self.bucket_sz = buffer_size//num_buckets # Ensure that num_buckets is divisible by buffer_size\n",
    "            self.num_buckets = num_buckets\n",
    "            self.p = p\n",
    "            ReplayBuffer.__init__(self, buffer_size=buffer_size, batch_sz=batch_sz)\n",
    "            \n",
    "        def sample(self):\n",
    "            while True:\n",
    "                sample_idx = random.randint(0,min(self.bucket_sz-1,self.cur_size-1))\n",
    "                bucket_idx = -1\n",
    "                bucket_lim = min(int(math.ceil(self.cur_size/self.bucket_sz)),self.num_buckets)\n",
    "\n",
    "                while not(bucket_idx>=0 and bucket_idx<bucket_lim):\n",
    "                    bucket_idx = int(np.random.geometric(self.p))-1\n",
    "\n",
    "                idx = (self.pointer - 1 - self.bucket_sz*(bucket_idx) - sample_idx)%self.buffer_size     \n",
    "\n",
    "                if idx >= self.cur_size:\n",
    "                    continue\n",
    "\n",
    "                transition = self.buffer[idx]\n",
    "                return idx, transition    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \n",
    "    def __init__(self,path,name):\n",
    "        self.path = BASE_PATH + path\n",
    "        self.name = name\n",
    "        self.font = cv2.FONT_HERSHEY_PLAIN\n",
    "        self.visualize_mem_steps = 0\n",
    "        self.snapshot_fig, self.snapshot_ax = plt.subplots()\n",
    "        self.logs={}\n",
    "        self.create()\n",
    "        self.setup()\n",
    "        \n",
    "    def create(self):\n",
    "        os.makedirs(self.path,exist_ok=True)\n",
    "        os.makedirs(self.path+self.name.split(\".\")[0]+\"-memory/\",exist_ok=True)\n",
    "        open(self.path+self.name, 'ab').close()\n",
    "        \n",
    "    def make_plot(self):\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.set_title(\"Train Log - Epsiode Returns\")\n",
    "        ax1.plot(self.logs[\"returns\"])\n",
    "        ax1.set_xlabel(\"Episodes\")\n",
    "        ax1.set_ylabel(\"Returns\")\n",
    "        fig1.savefig(self.path+self.name+'-returns.png',dpi=800)\n",
    "        ax1.clear()\n",
    "        \n",
    "#         fig2, ax2 = plt.subplots()\n",
    "        ax1.set_title(\"Train Log - Epsiode Variance\")\n",
    "        ax1.plot(self.logs[\"variance\"])\n",
    "        ax1.set_xlabel(\"Episodes\")\n",
    "        ax1.set_ylabel(\"Variance\")\n",
    "        fig1.savefig(self.path+self.name+'-variance.png',dpi=800)\n",
    "        ax1.clear()\n",
    "        \n",
    "#         fig3, ax3 = plt.subplots()\n",
    "        ax1.set_title(\"Train Log - Epsiode Entropy\")\n",
    "        ax1.plot(self.logs[\"entropy\"])\n",
    "        ax1.set_xlabel(\"Episodes\")\n",
    "        ax1.set_ylabel(\"Entropy\")\n",
    "        fig1.savefig(self.path+self.name+'-entropy.png',dpi=800)\n",
    "        ax1.clear()\n",
    "        \n",
    "#         fig4, ax4 = plt.subplots()\n",
    "        ax1.set_title(\"Train Log - Epsiode Running Variance\")\n",
    "        ax1.plot(self.logs[\"running_variance\"])\n",
    "        ax1.set_xlabel(\"Episodes\")\n",
    "        ax1.set_ylabel(\"Running Variance\")\n",
    "        fig1.savefig(self.path+self.name+'-runningvariance.png',dpi=800)\n",
    "        fig1.clf()\n",
    "        plt.close(fig1)\n",
    "                \n",
    "    def setup(self):\n",
    "        self.logs[\"returns\"] = []\n",
    "        self.logs[\"variance\"] = []\n",
    "        self.logs[\"running_variance\"] = []\n",
    "        self.logs[\"entropy\"] = []\n",
    "        \n",
    "    def step(self,ep_stats):\n",
    "        self.logs[\"returns\"].append(ep_stats[\"return\"])\n",
    "        self.logs[\"variance\"].append(ep_stats[\"variance\"])\n",
    "        self.logs[\"running_variance\"].append(ep_stats[\"running_variance\"])\n",
    "        self.logs[\"entropy\"].append(ep_stats[\"entropy\"])\n",
    "        \n",
    "    def save(self):\n",
    "        if len(self.logs[\"returns\"])%EPISODES==0 and len(self.logs[\"returns\"])>0:\n",
    "            self.make_plot()\n",
    "        with open(self.path+self.name,\"wb\") as f:\n",
    "            pickle.dump(self.logs,f)\n",
    "            \n",
    "    def save_replay_snapshot(self,histogram,episode):\n",
    "        self.snapshot_ax.hist2d(histogram[\"x\"], histogram[\"y\"], bins =[grid_dim, grid_dim], range=[[0, grid_dim-1], [0,grid_dim-1]])\n",
    "        \n",
    "        self.snapshot_fig.gca().invert_yaxis()\n",
    "        self.snapshot_fig.canvas.draw()\n",
    "\n",
    "        data = np.frombuffer(self.snapshot_fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        data = data.reshape(self.snapshot_fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        cv2.putText(data, \"Episode=\"+str(episode), (5,15), self.font, 1, (0, 255, 0), 1,)\n",
    "        \n",
    "        im_name = str(self.visualize_mem_steps)\n",
    "        im_name = (6-len(im_name))*\"0\" + im_name + \"_\" + str(episode)\n",
    "        save_path = self.path+self.name.split(\".\")[0]+\"-memory/\"+im_name+'.jpg'\n",
    "        cv2.imwrite(save_path,data)\n",
    "        \n",
    "        self.visualize_mem_steps += 1\n",
    "        self.snapshot_ax.clear()\n",
    "            \n",
    "    def load(self):\n",
    "        self.logs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self,state_space=(128,128),action_space=4,epsilon=0.1,decay_factor=0.7,decay_freq=100,base_epsilon=0.8,min_epsilon=0.1):\n",
    "        self.policy = np.zeros((*state_space,action_space))\n",
    "        self.action_space = action_space\n",
    "        self.epsilon = epsilon\n",
    "        self.decay_factor = decay_factor\n",
    "        self.decay_freq = decay_freq\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.base_epsilon = base_epsilon\n",
    "        \n",
    "    def update_epsilon(self,step):\n",
    "        self.epsilon = max(self.base_epsilon*self.decay_factor**(step//self.decay_freq),self.min_epsilon)\n",
    "        \n",
    "    def greedy_action(self,state):\n",
    "        x,y = state\n",
    "        return np.argmax(self.policy[x,y,:])\n",
    "    \n",
    "    def epsilon_greedy_action(self,state):\n",
    "        x,y = state\n",
    "        greedy = np.random.uniform() > self.epsilon\n",
    "        if greedy:\n",
    "            return np.argmax(self.policy[x,y,:])\n",
    "        return np.random.randint(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTracker:\n",
    "    \n",
    "    def __init__(self,q_opt,state_space=(128,128),action_space=4,max_history=50):\n",
    "        self.max_history = max_history\n",
    "        self.q_history = np.zeros((*state_space,action_space,max_history))\n",
    "        self.q_pointer = np.ones((*state_space,action_space,2))\n",
    "        self.q_opt = q_opt\n",
    "        \n",
    "    def step(self, q_value, state, action):\n",
    "        ptr_idx = (state[0],state[1],action,0)\n",
    "        history_idx = (state[0],state[1],action,int(self.q_pointer[ptr_idx]%self.max_history))\n",
    "\n",
    "        self.q_history[history_idx]=q_value\n",
    "        self.q_pointer[ptr_idx] = (self.q_pointer[ptr_idx]+1)%self.max_history\n",
    "        if self.q_pointer[ptr_idx] == 0:\n",
    "            self.q_pointer[state[0],state[1],action,1] = 0 # implies that max_history samples have been populated\n",
    "        \n",
    "    def get_running_mean(self,state,action):\n",
    "        samples = []\n",
    "        if self.q_pointer[(*state,action,1)] == 1:\n",
    "            samples = self.q_history[state[0],state[1],action,0:int(self.q_pointer[state[0],state[1],action,0])]\n",
    "        else:\n",
    "            samples = self.q_history[state[0],state[1],action,:]\n",
    "        expected_target = np.sum(samples)\n",
    "        return expected_target\n",
    "    \n",
    "    def get_optimal_mean(self,state,action):\n",
    "        return self.q_opt[state[0],state[1],action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.n = config[\"n\"]\n",
    "        self.batch_sz = config[\"batch_sz\"]\n",
    "        self.alpha = config[\"alpha\"]\n",
    "        self.epsilon = config[\"epsilon\"]\n",
    "        self.convergence_threshold = config[\"convergence_threshold\"]\n",
    "        self.buffer_sz = config[\"buffer_sz\"]\n",
    "        self.gridsize = config[\"gridsize\"]\n",
    "        self.max_history = config[\"max_history\"]\n",
    "        self.visualize_memory = config[\"visualize_memory\"]\n",
    "        self.experiment = config[\"experiment\"]\n",
    "        self.action_space = config['action_space']\n",
    "        self.noise_prob = config[\"noise_prob\"]\n",
    "        self.config = config\n",
    "        self.ep_stats = {}\n",
    "        self.n_step_history = [0 for i in range(self.n+1)]\n",
    "        self.n_step_ptr = 0\n",
    "        self.s1,self.s2,self.s3=0,0,0\n",
    "        \n",
    "    def setup(self):\n",
    "        self.agent = Agent(state_space=(self.gridsize,self.gridsize),action_space=self.config['action_space'], epsilon=self.epsilon)\n",
    "        if self.config[\"experiment\"] == \"Geometric-Decay\":\n",
    "            self.memory = BucketedGeometricDecayReplayBuffer(buffer_size=self.buffer_sz,batch_sz=self.batch_sz,p=self.config[\"geometric_p\"],num_buckets=self.config[\"geometric_buckets\"])\n",
    "        else:\n",
    "            self.memory = ReplayBuffer(buffer_size=self.buffer_sz,batch_sz=self.batch_sz)\n",
    "        self.env = Environment(gridsize=self.gridsize)\n",
    "        self.logger = Logger(self.experiment+\"/\"+self.method+\"/\"+str(self.n)+\"/\",str(self.buffer_sz)+\".pickle\")\n",
    "        self.q_tracker = QTracker(self.env.compute_optimal_q_values(),state_space=(self.gridsize,self.gridsize),action_space=self.config['action_space'],max_history=self.config[\"max_history\"])  \n",
    "        \n",
    "    def decay_alpha(self,eps):\n",
    "        self.alpha = max(self.alpha*(0.7**(eps//DECAY_FREQ)),0.0001)\n",
    "        \n",
    "    def decay_noise(self,eps):\n",
    "        self.noise_prob =  self.config[\"noise_prob\"]*(self.config[\"noise_decay\"]**(eps//self.config[\"noise_decay_freq\"]))\n",
    "        \n",
    "    def reset_episode_log(self):\n",
    "        self.ep_stats[\"return\"] = 0\n",
    "        self.ep_stats[\"variance\"] = 0\n",
    "        self.ep_stats[\"running_variance\"] = 0\n",
    "        self.ep_stats[\"entropy\"] = 0\n",
    "        self.ep_stats[\"train_step\"] = 0\n",
    "        \n",
    "    def update_episode_log(self,reward,variance,running_variance,entropy):\n",
    "        self.ep_stats[\"train_step\"] += 1\n",
    "        self.ep_stats[\"return\"] += reward\n",
    "        self.ep_stats[\"variance\"] += variance\n",
    "        self.ep_stats[\"running_variance\"] += running_variance\n",
    "        self.ep_stats[\"entropy\"] += entropy\n",
    "        \n",
    "    def process_episode_log(self):\n",
    "        self.ep_stats[\"running_variance\"] = self.ep_stats[\"running_variance\"]/self.ep_stats[\"train_step\"]\n",
    "        self.ep_stats[\"variance\"] = self.ep_stats[\"variance\"]/self.ep_stats[\"train_step\"]\n",
    "        self.ep_stats[\"entropy\"] = self.ep_stats[\"entropy\"]/self.ep_stats[\"train_step\"]     \n",
    "        \n",
    "    def reset_n_step_buffer(self):\n",
    "        self.n_step_history = [0 for i in range(self.n+1)]\n",
    "        self.n_step_ptr = 0\n",
    "        \n",
    "    def update_n_step_buffer(self,action,next_state,reward):\n",
    "        self.n_step_history[self.n_step_ptr] = (next_state,action,reward)\n",
    "        self.n_step_ptr = (self.n_step_ptr+1)%(self.n+1)\n",
    "        \n",
    "    def save_q_values(self):\n",
    "        with open(BASE_PATH+self.experiment+\"/\"+self.method+\"/\"+str(self.n)+\"/\"+str(self.buffer_sz)+\"-q.pickle\",\"wb\") as f:\n",
    "            pickle.dump(self.agent.policy,f)\n",
    "            \n",
    "    def get_action_probability(self,state,action):\n",
    "        if action == np.argmax(self.agent.policy[state[0],state[1],:]):\n",
    "            return 1 - self.agent.epsilon + self.agent.epsilon/self.agent.action_space\n",
    "        return self.agent.epsilon/self.agent.action_space\n",
    "    \n",
    "    def get_entropy(self,q_vec):\n",
    "        entropy = 0\n",
    "        abs_max = float('-inf')\n",
    "        for i in range(q_vec.shape[0]):\n",
    "            if abs(q_vec[i]) > abs_max:\n",
    "                abs_max = abs(q_vec[i])\n",
    "                \n",
    "        for i in range(q_vec.shape[0]):\n",
    "            norm = q_vec[i] / (abs_max+10**-5)\n",
    "            entropy += -norm*math.exp(norm)\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    def get_variance(self,state,action,target,q_val):\n",
    "        self.q_tracker.step(q_val,state,action)\n",
    "        running_expectation = self.q_tracker.get_running_mean(state,action)\n",
    "        expectation = self.q_tracker.get_optimal_mean(state,action)\n",
    "        variance = (target-expectation)**2\n",
    "        running_variance = (target-running_expectation)**2\n",
    "        return variance, running_variance\n",
    "            \n",
    "    def learn(self):\n",
    "        for eps in tqdm(range(EPISODES)):\n",
    "            cur_state = self.env.reset()\n",
    "            self.reset_episode_log()\n",
    "            done = False\n",
    "            self.reset_n_step_buffer()\n",
    "            self.update_n_step_buffer(0,cur_state,-1)\n",
    "            self.decay_alpha(eps)\n",
    "            while not done and self.env.get_steps()<TIMEOUT:\n",
    "                action = self.agent.epsilon_greedy_action(cur_state)\n",
    "                if not self.memory.at_quorom():\n",
    "                    action = random.randint(0,3)\n",
    "                reward, next_state, done = self.env.step(action)\n",
    "                self.update_n_step_buffer(action,next_state,reward)\n",
    "                transition = (*self.make_transition(),done)\n",
    "                self.memory.addx(transition)\n",
    "                if not self.memory.at_quorom():\n",
    "                    continue\n",
    "                batch = self.memory.sample_batch()\n",
    "                variance,running_variance,entropy = self.train_on_batch(batch)    \n",
    "                cur_state = next_state\n",
    "                self.update_episode_log(reward,variance,running_variance,entropy)\n",
    "    \n",
    "            self.process_episode_log()\n",
    "            self.logger.step(self.ep_stats)\n",
    "            self.logger.save()\n",
    "        self.save_q_values()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nStepRunner(Runner):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.method = \"TD\"\n",
    "        Runner.__init__(self,config)\n",
    "        \n",
    "    def uncorrected_n_step(self,start_ptr):\n",
    "        reward = 0\n",
    "        cur_gamma = 1\n",
    "        for ptr in range(start_ptr+1,start_ptr+1+self.n):\n",
    "            _,state,r = self.n_step_history[ptr%(self.n+1)]\n",
    "            if state is None: # Truncate reward as goal reached\n",
    "                break\n",
    "            reward += r*cur_gamma\n",
    "            cur_gamma = cur_gamma*self.gamma\n",
    "        return reward\n",
    "           \n",
    "    def make_transition(self):\n",
    "        start_ptr = (self.n_step_ptr)%(self.n+1)\n",
    "        start_state,_,_ = self.n_step_history[start_ptr]\n",
    "        _,action,_ = self.n_step_history[(start_ptr+1)%(self.n+1)]\n",
    "        reward = self.uncorrected_n_step(start_ptr)\n",
    "        final_state,_,_ = self.n_step_history[(start_ptr-1)%(self.n+1)]\n",
    "        transition = (start_state,action,reward,final_state)\n",
    "        return transition\n",
    "        \n",
    "    def get_TD_n_target(self,reward,q_next):\n",
    "        target = reward + (self.gamma**self.n)*q_next\n",
    "        return target     \n",
    "    \n",
    "    def refresh_transition(self,state):\n",
    "        cur_state = state\n",
    "        cur_action, next_state, done = None, None, False\n",
    "        reward, cur_gamma = 0, 1\n",
    "        for step in range(self.n):\n",
    "            action = self.agent.epsilon_greedy_action(cur_state)\n",
    "            if step == 0:\n",
    "                cur_action = action\n",
    "            r, next_state, done = self.env.step_async(cur_state,action)\n",
    "            if \"Noisy\" in self.experiment:\n",
    "                if random.random() < self.noise_prob:\n",
    "                    r, next_state, done = self.env.step_async(cur_state,random.randint(0,self.config[\"action_space\"]-1))\n",
    "            reward += r*cur_gamma\n",
    "            cur_gamma = cur_gamma*self.gamma\n",
    "            cur_state = next_state\n",
    "        return cur_action, reward, next_state, done\n",
    "    \n",
    "    def train_on_batch(self,batch):\n",
    "        variance,running_variance,entropy = 0,0,0\n",
    "        for transition in batch:\n",
    "            cur_state,action,reward,next_state,done = transition\n",
    "            if \"Refresh\" in self.experiment:\n",
    "                action,reward,next_state,done = self.refresh_transition(cur_state)\n",
    "            q_cur = self.agent.policy[cur_state[0],cur_state[1],action]\n",
    "            q_next = 0\n",
    "            if next_state is not None: # Episode in progress, No truncation needed\n",
    "                next_action = self.agent.greedy_action(next_state)\n",
    "                q_next = self.agent.policy[next_state[0],next_state[1],next_action]\n",
    "            target = self.get_TD_n_target(reward,q_next)\n",
    "            self.agent.policy[cur_state[0],cur_state[1],action] = q_cur + self.alpha*(target - q_cur)\n",
    "            # Updating stats\n",
    "            if next_state is not None:\n",
    "                entropy += self.get_entropy(self.agent.policy[next_state[0],next_state[1],:])\n",
    "            v, r_v = self.get_variance(cur_state,action,target,self.agent.policy[cur_state[0],cur_state[1],action])\n",
    "            variance += v\n",
    "            running_variance += r_v\n",
    "            \n",
    "        return variance/len(batch),running_variance//len(batch),entropy/len(batch)\n",
    "        \n",
    "    def learn_n_step(self):\n",
    "                \n",
    "        total_steps = 0\n",
    "            \n",
    "        for eps in tqdm(range(EPISODES)):\n",
    "            cur_state = self.env.reset()\n",
    "            self.reset_episode_log()\n",
    "            self.decay_alpha(eps)\n",
    "            self.decay_noise(eps)\n",
    "            self.reset_n_step_buffer()\n",
    "            self.update_n_step_buffer(0,cur_state,-1)\n",
    "            T = TIMEOUT\n",
    "            done = False\n",
    "            reward = 0\n",
    "            \n",
    "            while True:\n",
    "                                \n",
    "                time_step = self.env.get_steps()\n",
    "                \n",
    "                if time_step == TIMEOUT:\n",
    "                    break\n",
    "                    \n",
    "                # Not timeout or termination\n",
    "                if time_step < T:\n",
    "                    action = self.agent.epsilon_greedy_action(cur_state)\n",
    "                    if not self.memory.at_quorom():\n",
    "                        action = random.randint(0,3)\n",
    "                    reward, next_state, done = self.env.step(action)\n",
    "                    cur_state = next_state\n",
    "                    self.update_n_step_buffer(action,next_state,reward)\n",
    "                    # reached terminal state\n",
    "                    if done:\n",
    "                        T = time_step\n",
    "                    # incrementing total env steps\n",
    "                    total_steps += 1\n",
    "                \n",
    "                # Not timeout\n",
    "                elif time_step != TIMEOUT:\n",
    "                    time_step += 1    \n",
    "                    self.update_n_step_buffer(-1,None,0)\n",
    "                    \n",
    "                if self.visualize_memory and total_steps%MEMORY_VISUALIZE_FREQ[self.buffer_sz]==0:\n",
    "                    memory_histogram = self.memory.get_buffer_histogram()\n",
    "                    self.logger.save_replay_snapshot(memory_histogram,eps+1)\n",
    "                        \n",
    "                tau = time_step - (self.n - 1)\n",
    "                \n",
    "                if tau >= 0:\n",
    "                    transition = (*self.make_transition(),done)\n",
    "                    self.memory.add(transition)\n",
    "                    \n",
    "                    if not self.memory.at_quorom():\n",
    "                        continue\n",
    "                    \n",
    "                    batch = self.memory.sample_batch()\n",
    "                    if 'CER' in self.config[\"experiment\"]:\n",
    "                        batch[-1] = transition\n",
    "                    variance,running_variance,entropy = self.train_on_batch(batch)\n",
    "                    self.update_episode_log(reward,variance,running_variance,entropy)\n",
    "                    \n",
    "                if tau >= T:\n",
    "                    break\n",
    "                                             \n",
    "\n",
    "            if not self.memory.at_quorom():\n",
    "                continue\n",
    "                    \n",
    "            self.process_episode_log()\n",
    "            self.logger.step(self.ep_stats)\n",
    "            self.logger.save()\n",
    "        self.save_q_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Backup - State Trajectory and Replay Transition Scheme\n",
    "\n",
    "### Given reward steps - _n_ and  current state - _S0_\n",
    "Current Action = A0 <br>\n",
    "State Trajectory = { (S1,A1), (S2,A2), ... , (Sn) } <br>\n",
    "Rewards = { R1, R2, .... Rn }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class treeRunner(Runner):\n",
    "    \n",
    "    def __init__(self,config):\n",
    "        self.method = \"tree\"\n",
    "        Runner.__init__(self,config)\n",
    "        \n",
    "    def refresh_transition(self,cur_state):\n",
    "        state = cur_state\n",
    "        action = self.agent.epsilon_greedy_action(state)\n",
    "        rewards = []\n",
    "        state_trajectory = []\n",
    "        \n",
    "        for step in range(self.n): \n",
    "            reward, next_state, done = self.env.step_async(state,action)\n",
    "            if \"Noisy\" in self.experiment:\n",
    "                if random.random() < self.noise_prob:\n",
    "                    reward, next_state, done = self.env.step_async(state,random.randint(0,self.config[\"action_space\"]-1))\n",
    "            next_action = self.agent.epsilon_greedy_action(next_state)\n",
    "            rewards.append(reward)\n",
    "            state_trajectory.append([next_state,next_action])\n",
    "            \n",
    "        state_trajectory[-1] = [state_trajectory[-1][0]]\n",
    "        return action,rewards,state_trajectory,done\n",
    "        \n",
    "    def make_transition(self):\n",
    "        start_ptr = (self.n_step_ptr)%(self.n+1)\n",
    "        start_state,_,_ = self.n_step_history[start_ptr]\n",
    "        _,action,_ = self.n_step_history[(start_ptr+1)%(self.n+1)]\n",
    "        rewards = []\n",
    "        state_trajectory = []\n",
    "        for ptr in range(start_ptr+1,start_ptr+1+self.n):\n",
    "            state,prev_action,r = self.n_step_history[ptr%(self.n+1)]\n",
    "            if state is None: # Truncate reward as goal reached\n",
    "                break\n",
    "            if len(state_trajectory) > 0:\n",
    "                state_trajectory[-1].append(prev_action)\n",
    "            rewards.append(r)\n",
    "            state_trajectory.append([state])           \n",
    "        transition = (start_state,action,rewards,state_trajectory)\n",
    "        return transition\n",
    "    \n",
    "    # Σa π(a|s)Q(s,a)\n",
    "    def get_state_expectation(self,state): \n",
    "        expectation = 0\n",
    "        for action in range(4):\n",
    "            expectation += self.get_action_probability(state,action)*self.agent.policy[state[0],state[1],action]\n",
    "        return expectation\n",
    "        \n",
    "    def get_tree_target(self,rewards,state_trajectory):\n",
    "        target = 0\n",
    "        cur_gamma = 1\n",
    "        policy_prob = 1\n",
    "        for lvl in range(len(state_trajectory)):\n",
    "            target += cur_gamma*policy_prob*rewards[lvl]\n",
    "            cur_gamma *= self.gamma   \n",
    "            next_state = state_trajectory[lvl][0]\n",
    "            if next_state is None: # truncate target since goal has been reached\n",
    "                break \n",
    "            elif len(state_trajectory[lvl]) == 1: # final state\n",
    "                target += cur_gamma*policy_prob*self.get_state_expectation(next_state)\n",
    "            else: # intermediate states\n",
    "                next_action = state_trajectory[lvl][1]\n",
    "                target += cur_gamma*policy_prob*self.get_state_expectation(next_state)\n",
    "                policy_prob *= self.get_action_probability(next_state,next_action)\n",
    "                target -= cur_gamma*policy_prob*self.agent.policy[next_state[0],next_state[1],next_action]\n",
    "\n",
    "        return target     \n",
    "    \n",
    "    def train_on_batch(self,batch):\n",
    "        variance,running_variance,entropy,ctr = 0,0,0,0\n",
    "        for transition in batch:\n",
    "            cur_state,action,rewards,state_trajectory,done = transition\n",
    "            if \"Refresh\" in self.experiment:\n",
    "                action,rewards,state_trajectory,done = self.refresh_transition(cur_state)\n",
    "            q_cur = self.agent.policy[cur_state[0],cur_state[1],action]\n",
    "            final_state = state_trajectory[-1][0]\n",
    "            target = self.get_tree_target(rewards,state_trajectory)\n",
    "            self.agent.policy[cur_state[0],cur_state[1],action] = q_cur + self.alpha*(target - q_cur)\n",
    "            # Updating stats\n",
    "            if final_state is not None:\n",
    "                entropy += self.get_entropy(self.agent.policy[final_state[0],final_state[1],:])\n",
    "            v, r_v = self.get_variance(cur_state,action,target,self.agent.policy[cur_state[0],cur_state[1],action])\n",
    "            variance += v\n",
    "            running_variance += r_v\n",
    "            ctr += 1\n",
    "            \n",
    "        return variance/ctr,running_variance/ctr,entropy/ctr\n",
    "        \n",
    "    def learn_n_step(self):\n",
    "        \n",
    "        total_steps = 0\n",
    "                \n",
    "        for eps in tqdm(range(EPISODES)):\n",
    "            cur_state = self.env.reset()\n",
    "            self.reset_episode_log()\n",
    "            self.decay_alpha(eps)\n",
    "            self.decay_noise(eps)\n",
    "            self.reset_n_step_buffer()\n",
    "            self.update_n_step_buffer(0,cur_state,-1)\n",
    "            \n",
    "            T = TIMEOUT\n",
    "            done = False\n",
    "            reward = 0\n",
    "            \n",
    "            while True:\n",
    "                                \n",
    "                time_step = self.env.get_steps()\n",
    "                \n",
    "                if time_step == TIMEOUT:\n",
    "                    break\n",
    "                    \n",
    "                # Not timeout or termination\n",
    "                if time_step < T:\n",
    "                    action = self.agent.epsilon_greedy_action(cur_state)\n",
    "                    if not self.memory.at_quorom():\n",
    "                        action = random.randint(0,3)\n",
    "                    reward, next_state, done = self.env.step(action)\n",
    "                    cur_state = next_state\n",
    "                    self.update_n_step_buffer(action,next_state,reward)\n",
    "                    # reached terminal state\n",
    "                    if done:\n",
    "                        T = time_step\n",
    "                    # incrementing total env steps\n",
    "                    total_steps += 1\n",
    "                \n",
    "                # Not timeout\n",
    "                elif time_step != TIMEOUT:\n",
    "                    time_step += 1    \n",
    "                    self.update_n_step_buffer(-1,None,0)\n",
    "                    \n",
    "                if self.visualize_memory and total_steps%MEMORY_VISUALIZE_FREQ[self.buffer_sz]==0:\n",
    "                    memory_histogram = self.memory.get_buffer_histogram()\n",
    "                    self.logger.save_replay_snapshot(memory_histogram,eps+1)\n",
    "                        \n",
    "                tau = time_step - (self.n - 1)\n",
    "                \n",
    "                if tau >= 0:\n",
    "                    transition = (*self.make_transition(),done)\n",
    "                    self.memory.add(transition)\n",
    "                    \n",
    "                    if not self.memory.at_quorom():\n",
    "                        continue\n",
    "                    \n",
    "                    batch = self.memory.sample_batch()\n",
    "                    \n",
    "                    if 'CER' in self.config[\"experiment\"]:\n",
    "                        batch[-1] = transition\n",
    "                        \n",
    "                    variance,running_variance,entropy = self.train_on_batch(batch)\n",
    "                    self.update_episode_log(reward,variance,running_variance,entropy)\n",
    "                    \n",
    "                if tau >= T:\n",
    "                    break\n",
    "                                             \n",
    "\n",
    "            if not self.memory.at_quorom():\n",
    "                continue\n",
    "                    \n",
    "            self.process_episode_log()\n",
    "            self.logger.step(self.ep_stats)\n",
    "            self.logger.save()\n",
    "        self.save_q_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"experiment\":\"CER-Refresh\", # \"Uniform / Refresh / Refresh-Noisy / Geometric-Decay / Small-Big / CER / CER-Refresh / PER\n",
    "    \"buffer_sz\":10000, # 100 / 1000 / 10000\n",
    "    \"n\":1, # 1 / 2 / 3\n",
    "    \n",
    "    \"gridsize\":grid_dim, # default = 64\n",
    "    \"action_space\":4, # default = 4\n",
    "    \"batch_sz\":10, # default = 10\n",
    "    \"alpha\":0.1, # default = 0.1\n",
    "    \"epsilon\":0.1, # default = 0.1\n",
    "    \"gamma\":0.99, # default = 0.99\n",
    "    \"max_history\":10, # default = 10\n",
    "    \"convergence_threshold\":10**-4, # default = 10**-4\n",
    "    \n",
    "    \"visualize_memory\":False,\n",
    "    \n",
    "    # Noisy Refresh Parameters\n",
    "    \"noise_prob\":0.5, # noise injection prob in noisy refresh regime\n",
    "    \"noise_decay\":0.5, # noise injection prob decay in noisy refresh regime\n",
    "    \"noise_decay_freq\":150,  # noise injection prob reduction frequency in noisy refresh regime\n",
    "    \n",
    "    # Geometric Decay Buffer Parameters\n",
    "    \"geometric_p\":0.11, # success probability of geometric distribution for Geometric Decay buffer\n",
    "    \"geometric_buckets\":10, # active buckets in geometric distribution for Geometric Decay buffer\n",
    "    \n",
    "    # Moving Geometric Decay Buffer Parameters\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3d260acc9a4431b48ac6316134a761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdbcf6c400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runner = nStepRunner(config)\n",
    "# runner = treeRunner(config)\n",
    "runner.setup()\n",
    "runner.learn_n_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f43dbfa8e9a4ed4a4f1d73bfd4283f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2acb54b46a14dc8b9e111e844f2ee34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305b0deedd214259bed02059f82383b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a883a000a3a9441aa34e36f01ff27858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5125b2abf2ba4078a5e09cbe8f99813f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3887b9670f4ab1b0943b39d2621f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fde1cbe44c446e58ce0a700cb30bffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aman M\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:528: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442558bff8364d2181d11a2a44cb8c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc9c783c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc1fe2630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc5efc3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdbcf4e630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc1e9fc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc1f09978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc0c05ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc0c057b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdbcb766d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdba6e1a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdbcb97518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc5eff780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc1f2e278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc0c05198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdbcf2c7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdcd9dc6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc1fdda90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc1f48400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdbd02fa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc0b43630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc5f17048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdbaaa45f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc0c052e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cdc0b43e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for exp in [\"CER-Refresh\",\"CER\"]:\n",
    "    for buff_sz in [100,10000]:\n",
    "        for n in [1,3]:\n",
    "            config['n'] = n \n",
    "            config['buffer_sz'] = buff_sz\n",
    "            config['experiment'] = exp\n",
    "            runner = treeRunner(config)\n",
    "            runner.setup()\n",
    "            runner.learn_n_step()\n",
    "\n",
    "# for buff_sz in [100,1000,10000]:\n",
    "#     for n in [1,2,3]:\n",
    "#         config['n'] = n \n",
    "#         config['buffer_sz'] = buff_sz\n",
    "#         runner = treeRunner(config)\n",
    "#         runner.setup()\n",
    "#         runner.lear n_n_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-123, -132, -121, -117, -138, -138, -125, -124, -126, -124, -146, -128, -123, -119, -128, -122, -127, -122, -121, -128]\n"
     ]
    }
   ],
   "source": [
    "print(runner.logger.logs[\"returns\"][-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997, -4997]\n"
     ]
    }
   ],
   "source": [
    "print(runner.logger.logs[\"returns\"][-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in range(4):\n",
    "#     for i in range(grid_dim):\n",
    "#         for j in range(grid_dim):\n",
    "#             print(\"% 1.1f\"%(runner.agent.policy[i,j,a]),end=\" \")\n",
    "#         print()\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "path = \"D:/Aman M/Videos/Burglary/vid6/\"\n",
    "outpath = \"D:/Aman M/Videos/Burglary/vid6_enhanced/\"\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(outpath,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = glob(path+\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1965/1965 [04:20<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "for img in tqdm(frames):\n",
    "    img_name = img.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    frame = cv2.imread(img)#cv2.cvtColor(,cv2.COLOR_BGR2GRAY)\n",
    "    grey_scale = frame[:,:,0]\n",
    "    enhanced = clahe.apply(grey_scale)\n",
    "    enhanced_frame = np.zeros((grey_scale.shape[0],grey_scale.shape[1],3))\n",
    "    enhanced_frame[:,:,0] = enhanced\n",
    "    enhanced_frame[:,:,1] = enhanced\n",
    "    enhanced_frame[:,:,2] = enhanced\n",
    "    cv2.imwrite(outpath+img_name+\".jpg\",enhanced_frame)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
